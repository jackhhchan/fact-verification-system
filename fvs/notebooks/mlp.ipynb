{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MLP\n",
    "Simple Multilayer Perceptron to use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-3.6086e-02,  5.2419e-02,  7.1797e-02,  5.5754e-02,  4.3034e-02,\n       -3.7642e-02,  3.1122e-02, -1.0537e-01, -8.5471e-02, -3.8272e-02,\n       -3.6829e-02,  7.1718e-02,  3.2325e-02,  5.0349e-02, -1.4943e-02,\n        3.4765e-02,  9.9398e-02,  1.0352e-01,  4.9882e-02, -9.2280e-02,\n       -4.8455e-02, -2.2770e-02, -1.1397e-01,  6.6670e-02,  4.6059e-02,\n       -4.8253e-03,  3.1790e-02,  8.5202e-02,  7.6544e-02, -5.6798e-02,\n        2.2006e-03,  1.0122e-01,  3.3008e-02,  1.6444e-02, -1.2835e-02,\n       -6.4925e-02,  7.4660e-02,  5.5163e-02,  2.6431e-04, -3.3244e-02,\n       -7.6224e-02, -1.9530e-01, -5.3684e-03, -2.2878e-02,  3.4950e-02,\n        5.2780e-02,  1.5911e-02, -8.4575e-02,  5.8473e-03,  1.3065e-01,\n        1.8299e-02,  1.6318e-02,  1.1358e-01, -1.1667e-01,  5.6145e-02,\n        2.8836e-03, -3.5630e-02,  3.9919e-02, -1.0776e-01, -4.8945e-02,\n        9.7135e-03,  1.3764e-02,  2.3084e-02,  8.7231e-03, -4.8894e-02,\n        9.1422e-02,  7.2054e-02, -5.0373e-03, -1.6506e-02, -8.7338e-02,\n       -5.9274e-02,  9.4040e-03,  9.1731e-02, -3.8402e-02,  2.5086e-03,\n       -4.5527e-02,  4.4838e-02, -3.5158e-02, -4.0651e-02,  7.0863e-02,\n       -1.4168e-02, -6.3670e-02,  1.3575e-01,  6.1447e-02, -1.4278e-01,\n       -9.1131e-02, -6.8539e-02, -2.8060e-02, -2.2723e-02,  8.8776e-02,\n       -5.2886e-02,  3.9971e-02, -1.4355e-01, -7.8233e-02,  1.3824e-01,\n        6.7299e-02,  6.0042e-02,  1.4758e-02, -1.0484e-01, -6.3575e-02,\n        6.1534e-02, -2.3862e-02, -4.8804e-02, -5.3596e-02, -1.3009e-01,\n       -5.4525e-02, -7.4910e-02, -4.7406e-02,  8.1201e-02, -9.3419e-03,\n       -8.4979e-02,  1.2964e-01, -5.7900e-02, -3.9798e-02, -2.3100e-02,\n        3.7799e-02, -7.8491e-02, -1.6548e-02,  7.6002e-02,  1.8375e-02,\n        1.5541e-02, -2.9713e-02, -5.5542e-03,  6.1104e-03,  6.7041e-03,\n        2.9687e-02,  4.6346e-02, -5.7867e-02,  1.1077e-02,  6.3857e-02,\n       -1.5175e-01,  2.2547e-02, -3.5672e-02, -2.4293e-02,  2.7051e-02,\n       -1.0513e-01,  1.1688e-01,  4.8011e-02, -5.0435e-03,  6.1730e-02,\n       -1.7159e-02, -1.7137e-02,  5.5547e-02,  8.7254e-02,  7.0929e-02,\n        8.3644e-03,  3.8948e-02, -4.7932e-02,  4.1949e-02,  2.4807e-02,\n       -3.0723e-03,  2.9492e-02,  2.6992e-02,  6.1049e-02, -6.5549e-02,\n       -6.0977e-02,  6.1356e-02, -7.9417e-02, -3.0936e-02, -2.0303e-02,\n        2.2522e-02, -2.4286e-04, -4.7917e-02,  3.0797e-02, -7.6871e-02,\n       -1.1919e-01, -2.6948e-03,  7.6130e-03, -5.3550e-02,  6.2585e-02,\n        1.1885e-02, -3.9176e-03, -5.9628e-02, -2.4811e-02, -1.2505e-01,\n        1.8545e-01, -4.4341e-03,  3.3983e-02,  6.1615e-02,  8.3662e-03,\n        3.0591e-02,  1.7040e-02,  1.4175e-02, -1.2342e-02,  9.5565e-03,\n       -1.0042e-01, -6.0025e-03,  2.0642e-01,  4.7904e-02, -3.3312e-02,\n       -9.0662e-02,  1.1536e-01, -1.1211e-01,  5.3523e-02, -4.9902e-02,\n       -1.0685e-02, -1.0038e-02,  2.3011e-02, -3.9378e-02,  2.4349e-02,\n       -2.5957e-01,  9.7387e-02, -1.1343e-02,  1.7740e-02,  3.5799e-03,\n       -4.5273e-02,  1.8633e-02, -1.0000e-02, -5.9936e-04, -3.5366e-02,\n       -1.2111e-02, -2.3065e-02,  6.7634e-02, -1.3071e-02,  1.1923e-01,\n       -5.8070e-03, -1.3193e-01, -7.7067e-02, -5.9427e-03, -9.1246e-02,\n        1.2442e-02, -9.8874e-02, -7.8730e-03,  1.1950e-01,  7.7528e-02,\n       -2.7252e-03,  1.9984e-02,  1.9633e-01,  6.9891e-02,  2.9573e-03,\n       -1.3447e-02, -1.0102e-01,  5.6106e-02, -1.2844e-02,  1.4991e-01,\n       -7.9613e-02, -6.2501e-03,  2.3635e-02, -1.0238e-02,  1.3821e-02,\n        4.1185e-02, -2.7416e-03,  1.2432e-01, -1.2179e-01, -2.4136e-02,\n        5.0373e-02,  4.1008e-02,  2.3094e-02, -3.3045e-02, -5.7082e-02,\n        2.0110e-01, -1.3705e-02,  3.4940e-02,  4.2047e-04,  3.2515e-03,\n       -9.7752e-02, -8.2250e-02, -2.2671e-02,  1.1099e-03, -1.3201e-01,\n        4.6892e-05,  2.0412e-03,  1.4106e-01, -3.6308e-02,  9.6831e-03,\n       -7.6405e-02,  5.6260e-02,  2.7786e-02,  4.2224e-02, -3.7315e-02,\n       -7.2890e-02,  2.8526e-02, -7.4274e-02, -6.7300e-02,  1.2026e-01,\n       -2.6406e-02, -5.0758e-03, -3.6782e-02, -6.7372e-03, -3.7099e-02,\n        3.5146e-02,  6.4354e-02,  1.1092e-02, -2.0435e-02, -1.3641e-01,\n       -3.5261e-02,  2.1310e-02, -6.2584e-02,  1.0597e-01, -9.3809e-03,\n       -1.2279e-02, -5.8217e-03, -2.9097e-02, -5.0029e-02,  3.2479e-02,\n        1.0079e-01,  7.4865e-02,  9.2050e-02, -6.3892e-04,  2.9422e-02],\n      dtype=float32)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# get word embeddings\n",
    "import numpy as np\n",
    "\n",
    "vectors = api.load('fasttext-wiki-news-subwords-300')\n",
    "vectors['hello']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "300"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vectors)\n",
    "vectors.vector_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'claim', 'label', 'evidence_title', 'evidence_sentence'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Unnamed: 0                                              claim     label  \\\n0           0  Nikolaj Coster-Waldau worked with the Fox Broa...  SUPPORTS   \n1           1  Nikolaj Coster-Waldau worked with the Fox Broa...  SUPPORTS   \n2           2                 Roman Atwood is a content creator.  SUPPORTS   \n\n             evidence_title                                  evidence_sentence  \n0  Fox_Broadcasting_Company  The Fox Broadcasting Company -LRB- often short...  \n1     Nikolaj_Coster-Waldau  He then played Detective John Amsterdam in the...  \n2              Roman_Atwood  He is best known for his vlogs , where he post...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>claim</th>\n      <th>label</th>\n      <th>evidence_title</th>\n      <th>evidence_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Nikolaj Coster-Waldau worked with the Fox Broa...</td>\n      <td>SUPPORTS</td>\n      <td>Fox_Broadcasting_Company</td>\n      <td>The Fox Broadcasting Company -LRB- often short...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Nikolaj Coster-Waldau worked with the Fox Broa...</td>\n      <td>SUPPORTS</td>\n      <td>Nikolaj_Coster-Waldau</td>\n      <td>He then played Detective John Amsterdam in the...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Roman Atwood is a content creator.</td>\n      <td>SUPPORTS</td>\n      <td>Roman_Atwood</td>\n      <td>He is best known for his vlogs , where he post...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../train.csv\")\n",
    "\n",
    "print(df.columns)\n",
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# possible augmentation - coreference resolution, titles -- then add this as another column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# basic cleaning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disenfranchise' 'disenfranchised' 'disengaged' 'disengagement'\n",
      " 'disenrolled' 'disfigured' 'disgorging' 'disgrace' 'disguise' 'disguises']\n"
     ]
    },
    {
     "data": {
      "text/plain": "(202563, 43113)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write the sentence encoder (tf idf weighted?)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "matrix = tfidf.fit_transform(df['evidence_sentence'])\n",
    "\n",
    "print(tfidf.get_feature_names_out()[11244:11254])\n",
    "matrix.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "11246"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_.get(\"disengaged\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fox Broadcasting Company -LRB- often shortened to Fox and stylized as FOX -RRB- is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox .\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0.05881791, 0.02334309, 0.00999008, 0.01384026, 0.        ,\n       0.04236151, 0.05370663, 0.05944368, 0.02334309, 0.00471161,\n       0.05670015, 0.00589628, 0.02334309, 0.        , 0.03050207,\n       0.00461607, 0.00447754, 0.02039734, 0.03381723, 0.01374472,\n       0.0099853 , 0.0584676 , 0.04098258, 0.05880995, 0.03050207,\n       0.04332644, 0.01057286, 0.05881791, 0.02334309, 0.0205088 ,\n       0.02591466, 0.05682276, 0.04230737, 0.00136938, 0.01187377,\n       0.02334309, 0.        ])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "def tfidf_weights(tokens: List[str]):\n",
    "    # from feature names out get the index\n",
    "    scores = np.zeros(len(tokens))\n",
    "    for i, token in enumerate(tokens):\n",
    "        scores[i] = tfidf.vocabulary_.get(token.lower())\n",
    "    scores = np.nan_to_num(scores, copy=False)\n",
    "    return scores / np.sum(scores)      # normalised\n",
    "\n",
    "sent = df['evidence_sentence'][0]\n",
    "print(sent)\n",
    "tfidf_weights(sent.split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He then played Detective John Amsterdam in the short-lived Fox television series New Amsterdam -LRB- 2008 -RRB- , as well as appearing as Frank Pike in the 2009 Fox television film Virtuality , originally intended as a pilot .\n",
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "DescribeResult(nobs=300, minmax=(-0.13246785661613103, 0.11091277527157217), mean=0.0008606826803999231, variance=0.0008328520100926018, skewness=-0.22878857512640563, kurtosis=4.465815401409027)"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "def encode(sent: str):\n",
    "    # 1. get tokens\n",
    "    sent = re.sub(rf\"[{punctuation}]\", '', sent)\n",
    "    tokens = sent.split()\n",
    "\n",
    "    # UPDATE: remove stop words to see if it converges\n",
    "    tokens = [t for t in tokens if t not in sw]\n",
    "    # weights = np.ones(len(tokens))\n",
    "    weights = tfidf_weights(tokens)\n",
    "\n",
    "    encoded = np.empty((len(tokens), vectors.vector_size))\n",
    "    for i, t in enumerate(tokens):\n",
    "        t = t.lower()\n",
    "        try:\n",
    "            v = vectors[t]\n",
    "        except:\n",
    "            v = np.zeros(vectors.vector_size)\n",
    "        encoded[i] = np.multiply(v, weights[i])\n",
    "    return np.sum(encoded, axis=0)\n",
    "\n",
    "sent = df['evidence_sentence'][1]\n",
    "print(sent)\n",
    "print(encode(sent).shape)\n",
    "from scipy.stats import describe\n",
    "describe(encode(sent))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "              0           1           2           3           4           5    \\\ncount  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \nmean     0.019157   -0.144394    0.015852    0.137294    0.017582   -0.093853   \nstd      0.101482    0.180438    0.123218    0.125897    0.136218    0.113029   \nmin     -0.399050   -0.987619   -0.158387   -0.172244   -0.451728   -0.526309   \n25%     -0.016076   -0.237129   -0.077598    0.065403   -0.050820   -0.132759   \n50%      0.019230   -0.100224   -0.002687    0.129528    0.000878   -0.052955   \n75%      0.074536   -0.041155    0.081869    0.180884    0.095905   -0.031380   \nmax      0.374026    0.224877    0.753815    0.567000    0.401476    0.117404   \n\n              6           7           8           9    ...         290  \\\ncount  100.000000  100.000000  100.000000  100.000000  ...  100.000000   \nmean     0.045612   -0.461705    0.082373    0.036538  ...   -0.014472   \nstd      0.089369    0.311769    0.197031    0.156420  ...    0.099175   \nmin     -0.154055   -1.613457   -0.274779   -0.393937  ...   -0.272612   \n25%     -0.001251   -0.631087   -0.034739   -0.035613  ...   -0.064821   \n50%      0.053616   -0.455672    0.054266    0.013974  ...   -0.012535   \n75%      0.103908   -0.256240    0.141267    0.117012  ...    0.043770   \nmax      0.221061    0.182474    0.918773    0.653752  ...    0.268188   \n\n              291         292         293         294         295         296  \\\ncount  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \nmean     0.009861   -0.109073   -0.007156   -0.020911    0.001668    0.054783   \nstd      0.095277    0.150877    0.123255    0.115458    0.098582    0.112653   \nmin     -0.249547   -0.609977   -0.545749   -0.413466   -0.241736   -0.280266   \n25%     -0.046887   -0.193663   -0.070080   -0.090097   -0.046632   -0.024143   \n50%     -0.003117   -0.081208   -0.001896   -0.013274    0.002956    0.075382   \n75%      0.052359   -0.002575    0.070761    0.049850    0.039971    0.112181   \nmax      0.335998    0.283057    0.294124    0.272128    0.523862    0.488391   \n\n              297         298         299  \ncount  100.000000  100.000000  100.000000  \nmean     0.075496    0.054024   -0.038319  \nstd      0.108010    0.094888    0.095547  \nmin     -0.162211   -0.217214   -0.317617  \n25%      0.021358    0.007385   -0.105858  \n50%      0.068907    0.072715   -0.055485  \n75%      0.117673    0.108643    0.041211  \nmax      0.579751    0.376641    0.260027  \n\n[8 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>...</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.019157</td>\n      <td>-0.144394</td>\n      <td>0.015852</td>\n      <td>0.137294</td>\n      <td>0.017582</td>\n      <td>-0.093853</td>\n      <td>0.045612</td>\n      <td>-0.461705</td>\n      <td>0.082373</td>\n      <td>0.036538</td>\n      <td>...</td>\n      <td>-0.014472</td>\n      <td>0.009861</td>\n      <td>-0.109073</td>\n      <td>-0.007156</td>\n      <td>-0.020911</td>\n      <td>0.001668</td>\n      <td>0.054783</td>\n      <td>0.075496</td>\n      <td>0.054024</td>\n      <td>-0.038319</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.101482</td>\n      <td>0.180438</td>\n      <td>0.123218</td>\n      <td>0.125897</td>\n      <td>0.136218</td>\n      <td>0.113029</td>\n      <td>0.089369</td>\n      <td>0.311769</td>\n      <td>0.197031</td>\n      <td>0.156420</td>\n      <td>...</td>\n      <td>0.099175</td>\n      <td>0.095277</td>\n      <td>0.150877</td>\n      <td>0.123255</td>\n      <td>0.115458</td>\n      <td>0.098582</td>\n      <td>0.112653</td>\n      <td>0.108010</td>\n      <td>0.094888</td>\n      <td>0.095547</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-0.399050</td>\n      <td>-0.987619</td>\n      <td>-0.158387</td>\n      <td>-0.172244</td>\n      <td>-0.451728</td>\n      <td>-0.526309</td>\n      <td>-0.154055</td>\n      <td>-1.613457</td>\n      <td>-0.274779</td>\n      <td>-0.393937</td>\n      <td>...</td>\n      <td>-0.272612</td>\n      <td>-0.249547</td>\n      <td>-0.609977</td>\n      <td>-0.545749</td>\n      <td>-0.413466</td>\n      <td>-0.241736</td>\n      <td>-0.280266</td>\n      <td>-0.162211</td>\n      <td>-0.217214</td>\n      <td>-0.317617</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.016076</td>\n      <td>-0.237129</td>\n      <td>-0.077598</td>\n      <td>0.065403</td>\n      <td>-0.050820</td>\n      <td>-0.132759</td>\n      <td>-0.001251</td>\n      <td>-0.631087</td>\n      <td>-0.034739</td>\n      <td>-0.035613</td>\n      <td>...</td>\n      <td>-0.064821</td>\n      <td>-0.046887</td>\n      <td>-0.193663</td>\n      <td>-0.070080</td>\n      <td>-0.090097</td>\n      <td>-0.046632</td>\n      <td>-0.024143</td>\n      <td>0.021358</td>\n      <td>0.007385</td>\n      <td>-0.105858</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.019230</td>\n      <td>-0.100224</td>\n      <td>-0.002687</td>\n      <td>0.129528</td>\n      <td>0.000878</td>\n      <td>-0.052955</td>\n      <td>0.053616</td>\n      <td>-0.455672</td>\n      <td>0.054266</td>\n      <td>0.013974</td>\n      <td>...</td>\n      <td>-0.012535</td>\n      <td>-0.003117</td>\n      <td>-0.081208</td>\n      <td>-0.001896</td>\n      <td>-0.013274</td>\n      <td>0.002956</td>\n      <td>0.075382</td>\n      <td>0.068907</td>\n      <td>0.072715</td>\n      <td>-0.055485</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.074536</td>\n      <td>-0.041155</td>\n      <td>0.081869</td>\n      <td>0.180884</td>\n      <td>0.095905</td>\n      <td>-0.031380</td>\n      <td>0.103908</td>\n      <td>-0.256240</td>\n      <td>0.141267</td>\n      <td>0.117012</td>\n      <td>...</td>\n      <td>0.043770</td>\n      <td>0.052359</td>\n      <td>-0.002575</td>\n      <td>0.070761</td>\n      <td>0.049850</td>\n      <td>0.039971</td>\n      <td>0.112181</td>\n      <td>0.117673</td>\n      <td>0.108643</td>\n      <td>0.041211</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.374026</td>\n      <td>0.224877</td>\n      <td>0.753815</td>\n      <td>0.567000</td>\n      <td>0.401476</td>\n      <td>0.117404</td>\n      <td>0.221061</td>\n      <td>0.182474</td>\n      <td>0.918773</td>\n      <td>0.653752</td>\n      <td>...</td>\n      <td>0.268188</td>\n      <td>0.335998</td>\n      <td>0.283057</td>\n      <td>0.294124</td>\n      <td>0.272128</td>\n      <td>0.523862</td>\n      <td>0.488391</td>\n      <td>0.579751</td>\n      <td>0.376641</td>\n      <td>0.260027</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 300 columns</p>\n</div>"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_encode(sents):\n",
    "    batch = np.zeros((len(sents), vectors.vector_size))\n",
    "    for i in range(len(batch)):\n",
    "        batch[i] = encode(sents.iloc[i])\n",
    "    return batch\n",
    "\n",
    "batch = batch_encode(df['claim'][:100])\n",
    "batch_df = pd.DataFrame(batch)\n",
    "batch_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORTS    146033\n",
      "REFUTES      56523\n",
      "Name: label, dtype: int64\n",
      "56523\n"
     ]
    },
    {
     "data": {
      "text/plain": "146033"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: balance the dataset\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "num_refutes = len(df[df['label'].str.contains('REFUTE') == True])\n",
    "print(num_refutes)\n",
    "\n",
    "support_indices = df[df['label'].str.contains('SUPPORT') == True].index\n",
    "len(support_indices)\n",
    "# randomly draw {num_refutes} amount from this index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied encoding... 2022-04-10 20:39:39.609122\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[ 0.0022, -0.0258,  0.0154,  0.0088,  0.0190, -0.0197, -0.0086, -0.0917,\n          -0.0023, -0.0234],\n         [ 0.0022, -0.0258,  0.0154,  0.0088,  0.0190, -0.0197, -0.0086, -0.0917,\n          -0.0023, -0.0234],\n         [-0.0015, -0.0599,  0.0691, -0.0066,  0.0265,  0.0033,  0.0086, -0.1273,\n           0.0243,  0.0355]]),\n tensor([[ 0.0056, -0.0247,  0.0173,  0.0148, -0.0021, -0.0274, -0.0056, -0.0850,\n           0.0022, -0.0029],\n         [-0.0015, -0.0249,  0.0139,  0.0255,  0.0107, -0.0208,  0.0048, -0.0991,\n           0.0108,  0.0234],\n         [ 0.0024, -0.0179,  0.0074, -0.0208,  0.0222, -0.0266, -0.0075, -0.1099,\n           0.0107,  0.0072]]))"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the claim and evidence sets\n",
    "import torch\n",
    "# print(len(df))\n",
    "# df = df.drop(labels=[157030, 157031, 160536], axis=0)\n",
    "# df = df.drop(labels=[96165, 96166, 188267], axis=0)\n",
    "df = df.drop(labels=[188264], axis=0)\n",
    "# print(len(df))\n",
    "\n",
    "train_claims = torch.tensor(df['claim'].apply(lambda s: encode(s)), dtype=torch.float32)\n",
    "train_evidence = torch.tensor(df['evidence_sentence'].apply(lambda s: encode(s)), dtype=torch.float32)\n",
    "from datetime import datetime\n",
    "print(f\"Applied encoding... {datetime.now()}\")\n",
    "\n",
    "train_claims[:3, :10], train_evidence[:3, :10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 188263\tText: Spider-Man is relatable.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1, 0)"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nans_in_claims = 0\n",
    "nans_in_evidence = 0\n",
    "for i in range(len(train_claims)):\n",
    "    if torch.isnan(train_claims[i]).any():\n",
    "        print(f\"Index: {i}\\tText: {df['claim'].iloc[i]}\")\n",
    "        nans_in_claims+=1\n",
    "    if torch.isnan(train_evidence[i]).any():\n",
    "        print(f\"Index: {i}\\tText: {df['evidence_sentence'].iloc[i]}\")\n",
    "        nans_in_evidence+=1\n",
    "\n",
    "nans_in_claims, nans_in_evidence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORTS 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "[tensor([[-2.3241e-01, -3.1076e-02,  1.3272e-01,  ..., -4.5082e-03,\n          -8.5342e-03, -2.4965e-02],\n         [-9.7410e-02, -2.5672e-01,  6.5306e-02,  ...,  2.3662e-01,\n          -2.7858e-02, -3.4912e-02],\n         [-1.6641e-01, -2.5534e-02, -8.4005e-02,  ...,  1.2521e-01,\n           8.5710e-02, -6.7112e-02],\n         ...,\n         [-1.0712e-01, -3.6259e-01, -1.1233e-01,  ..., -3.6882e-02,\n           9.3531e-02, -1.8838e-01],\n         [-2.1391e-03, -1.4190e-01,  1.9514e-01,  ...,  1.3104e-01,\n           7.0668e-02,  1.0340e-01],\n         [-1.1295e-01, -1.5569e-01, -3.3710e-02,  ...,  1.1398e-01,\n          -2.2742e-01, -1.9100e-04]]),\n tensor([[-0.3277, -0.1098,  0.3066,  ...,  0.1991,  0.2710,  0.1359],\n         [ 0.0822, -0.0989,  0.1281,  ...,  0.0185,  0.0387, -0.3607],\n         [ 0.0873, -0.2155, -0.1350,  ..., -0.1718,  0.0236,  0.0783],\n         ...,\n         [-0.2675, -0.8068,  0.2540,  ...,  0.2972,  0.6239,  0.2465],\n         [-0.3402, -0.5360,  0.1442,  ...,  0.2735,  0.4627,  0.2535],\n         [-0.3352, -0.6791, -0.2044,  ...,  0.5816,  0.1024, -0.1972]]),\n tensor([[0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [0., 1.],\n         [0., 1.],\n         [1., 0.],\n         [1., 0.],\n         [0., 1.],\n         [0., 1.],\n         [1., 0.],\n         [1., 0.],\n         [0., 1.]])]"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Train(Dataset):\n",
    "    def __init__(self, claims, evidences, labels):\n",
    "        self._claims = claims\n",
    "        self._evidences = evidences\n",
    "        self._labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim = self._claims[idx]\n",
    "        evidence = self._evidences[idx]\n",
    "        label = self._labels[idx]\n",
    "        return claim, evidence, label\n",
    "\n",
    "\n",
    "labels = pd.Categorical(df['label'])\n",
    "print(labels[0], labels.codes[0])\n",
    "labels = F.one_hot(torch.tensor(labels.codes, dtype=torch.long), num_classes=2).float()\n",
    "train = Train(claims=train_claims, evidences=train_evidence, labels=labels)\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=12, shuffle=True)\n",
    "data = next(iter(train_dl))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5078, 0.4922],\n",
      "        [0.5058, 0.4942],\n",
      "        [0.5079, 0.4921]], grad_fn=<SliceBackward0>) tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6970160603523254"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # debug cell\n",
    "# import torch.nn as nn\n",
    "# cri = nn.CrossEntropyLoss()\n",
    "#\n",
    "# model = Head(embedding_length=vectors.vector_size, interaction=False)\n",
    "# labels = data[2]\n",
    "# preds = model(data[0], data[1])\n",
    "# print(preds[:3], labels[:3])\n",
    "#\n",
    "# loss = cri(preds, labels)\n",
    "# loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1000] loss: 0.5941623095571995\n",
      "[1, 2000] loss: 0.5923449902236462\n",
      "[1, 3000] loss: 0.5950953635573387\n",
      "[1, 4000] loss: 0.5926783253252507\n",
      "[1, 5000] loss: 0.5909283387362957\n",
      "[1, 6000] loss: 0.5945949888527393\n",
      "[1, 7000] loss: 0.599261655330658\n",
      "[1, 8000] loss: 0.5923449884653091\n",
      "[1, 9000] loss: 0.5930949890613556\n",
      "[1, 10000] loss: 0.5950949883759021\n",
      "[1, 11000] loss: 0.5922616565227509\n",
      "[1, 12000] loss: 0.5931783213019372\n",
      "[1, 13000] loss: 0.583178324252367\n",
      "[1, 14000] loss: 0.5898449904918671\n",
      "[1, 15000] loss: 0.5907616551220417\n",
      "[1, 16000] loss: 0.5883450075089931\n",
      "[2, 1000] loss: 0.589511657267809\n",
      "[2, 2000] loss: 0.5884283211827278\n",
      "[2, 3000] loss: 0.5975116538107396\n",
      "[2, 4000] loss: 0.5970116551816463\n",
      "[2, 5000] loss: 0.5891783232688904\n",
      "[2, 6000] loss: 0.5825949899554252\n",
      "[2, 7000] loss: 0.5884283227026462\n",
      "[2, 8000] loss: 0.5985949896872044\n",
      "[2, 9000] loss: 0.5940949901044369\n",
      "[2, 10000] loss: 0.5905116545557976\n",
      "[2, 11000] loss: 0.5938449874520302\n",
      "[2, 12000] loss: 0.5842616584897041\n",
      "[2, 13000] loss: 0.5940116540789604\n",
      "[2, 14000] loss: 0.5977616572678089\n",
      "[2, 15000] loss: 0.5945949888825417\n",
      "[2, 16000] loss: 0.5932616553902627\n",
      "[3, 1000] loss: 0.5922616550624371\n",
      "[3, 2000] loss: 0.5903449915349483\n",
      "[3, 3000] loss: 0.5939283219873905\n",
      "[3, 4000] loss: 0.5922616553008556\n",
      "[3, 5000] loss: 0.5960116548240185\n",
      "[3, 6000] loss: 0.5895949897766113\n",
      "[3, 7000] loss: 0.5881783249080181\n",
      "[3, 8000] loss: 0.5937616561949253\n",
      "[3, 9000] loss: 0.5968449900746345\n",
      "[3, 10000] loss: 0.585344988912344\n",
      "[3, 11000] loss: 0.5887616571187974\n",
      "[3, 12000] loss: 0.5915116558074951\n",
      "[3, 13000] loss: 0.5928449895083904\n",
      "[3, 14000] loss: 0.5965116561055184\n",
      "[3, 15000] loss: 0.5949283209145069\n",
      "[3, 16000] loss: 0.600094988077879\n",
      "[4, 1000] loss: 0.5914283237159252\n",
      "[4, 2000] loss: 0.5937616549432277\n",
      "[4, 3000] loss: 0.585928322583437\n",
      "[4, 4000] loss: 0.5917616581916809\n",
      "[4, 5000] loss: 0.588928324341774\n",
      "[4, 6000] loss: 0.5948449901640415\n",
      "[4, 7000] loss: 0.5964283227324486\n",
      "[4, 8000] loss: 0.5942616550624371\n",
      "[4, 9000] loss: 0.5926783229708672\n",
      "[4, 10000] loss: 0.5921783216893673\n",
      "[4, 11000] loss: 0.593844989567995\n",
      "[4, 12000] loss: 0.5904283226132393\n",
      "[4, 13000] loss: 0.5907616560161114\n",
      "[4, 14000] loss: 0.5899283217489719\n",
      "[4, 15000] loss: 0.5915949890911579\n",
      "[4, 16000] loss: 0.59842832249403\n",
      "[5, 1000] loss: 0.5937616544365882\n",
      "[5, 2000] loss: 0.5947616533339024\n",
      "[5, 3000] loss: 0.5929283221662045\n",
      "[5, 4000] loss: 0.5957616565525532\n",
      "[5, 5000] loss: 0.5986783210039138\n",
      "[5, 6000] loss: 0.5841783227622509\n",
      "[5, 7000] loss: 0.595761656910181\n",
      "[5, 8000] loss: 0.5888449891209603\n",
      "[5, 9000] loss: 0.5923449893593788\n",
      "[5, 10000] loss: 0.5967616554796695\n",
      "[5, 11000] loss: 0.5982616535425186\n",
      "[5, 12000] loss: 0.5927616555690766\n",
      "[5, 13000] loss: 0.5861783231794834\n",
      "[5, 14000] loss: 0.5873449892103672\n",
      "[5, 15000] loss: 0.5877616574764252\n",
      "[5, 16000] loss: 0.5900949892103672\n",
      "[6, 1000] loss: 0.5909283227324486\n",
      "[6, 2000] loss: 0.5929283227324486\n",
      "[6, 3000] loss: 0.5958449884355068\n",
      "[6, 4000] loss: 0.5935116567313671\n",
      "[6, 5000] loss: 0.5940116548836232\n",
      "[6, 6000] loss: 0.5969283226728439\n",
      "[6, 7000] loss: 0.5908449876606464\n",
      "[6, 8000] loss: 0.5935949890911579\n",
      "[6, 9000] loss: 0.5929283226132392\n",
      "[6, 10000] loss: 0.5818449894189834\n",
      "[6, 11000] loss: 0.5926783203780651\n",
      "[6, 12000] loss: 0.5918449903428554\n",
      "[6, 13000] loss: 0.5978449874520302\n",
      "[6, 14000] loss: 0.5946783214509487\n",
      "[6, 15000] loss: 0.590511655330658\n",
      "[6, 16000] loss: 0.584928322404623\n",
      "[7, 1000] loss: 0.5869283230304718\n",
      "[7, 2000] loss: 0.5905116558074951\n",
      "[7, 3000] loss: 0.5905116554498673\n",
      "[7, 4000] loss: 0.593428322672844\n",
      "[7, 5000] loss: 0.5928449903428554\n",
      "[7, 6000] loss: 0.5942616585791111\n",
      "[7, 7000] loss: 0.5861783226430416\n",
      "[7, 8000] loss: 0.5938449889123439\n",
      "[7, 9000] loss: 0.5905949879288673\n",
      "[7, 10000] loss: 0.5917616578042507\n",
      "[7, 11000] loss: 0.5933449885547161\n",
      "[7, 12000] loss: 0.5855949905216694\n",
      "[7, 13000] loss: 0.5947616553008557\n",
      "[7, 14000] loss: 0.6016783204078674\n",
      "[7, 15000] loss: 0.601094988912344\n",
      "[7, 16000] loss: 0.5918449877798557\n",
      "[8, 1000] loss: 0.594261657834053\n",
      "[8, 2000] loss: 0.5968449901044369\n",
      "[8, 3000] loss: 0.5975116552710533\n",
      "[8, 4000] loss: 0.5882616574764252\n",
      "[8, 5000] loss: 0.590094989746809\n",
      "[8, 6000] loss: 0.5861783234775066\n",
      "[8, 7000] loss: 0.5925949887633324\n",
      "[8, 8000] loss: 0.5948449889123439\n",
      "[8, 9000] loss: 0.5895116575062275\n",
      "[8, 10000] loss: 0.588928323507309\n",
      "[8, 11000] loss: 0.5931783226430416\n",
      "[8, 12000] loss: 0.5952616550624371\n",
      "[8, 13000] loss: 0.5949283220767975\n",
      "[8, 14000] loss: 0.5895116553008556\n",
      "[8, 15000] loss: 0.5880116556584835\n",
      "[8, 16000] loss: 0.6005116541981697\n",
      "[9, 1000] loss: 0.5925949880778789\n",
      "[9, 2000] loss: 0.59634498873353\n",
      "[9, 3000] loss: 0.5960949890315532\n",
      "[9, 4000] loss: 0.5896783231794834\n",
      "[9, 5000] loss: 0.5917616551220417\n",
      "[9, 6000] loss: 0.5985949887037277\n",
      "[9, 7000] loss: 0.5915116566121579\n",
      "[9, 8000] loss: 0.587011656075716\n",
      "[9, 9000] loss: 0.5982616564631462\n",
      "[9, 10000] loss: 0.584011656165123\n",
      "[9, 11000] loss: 0.5937616570293903\n",
      "[9, 12000] loss: 0.5869283229112625\n",
      "[9, 13000] loss: 0.5898449896872043\n",
      "[9, 14000] loss: 0.5950116550028324\n",
      "[9, 15000] loss: 0.5953449879586696\n",
      "[9, 16000] loss: 0.5909283215999603\n",
      "[10, 1000] loss: 0.5933449876010418\n",
      "[10, 2000] loss: 0.5999283209443093\n",
      "[10, 3000] loss: 0.5917616548240184\n",
      "[10, 4000] loss: 0.5951783229112625\n",
      "[10, 5000] loss: 0.5901783216893673\n",
      "[10, 6000] loss: 0.5880116563439369\n",
      "[10, 7000] loss: 0.5900949906408787\n",
      "[10, 8000] loss: 0.5943449880480767\n",
      "[10, 9000] loss: 0.5947616564333439\n",
      "[10, 10000] loss: 0.5945116542875767\n",
      "[10, 11000] loss: 0.5916783230602741\n",
      "[10, 12000] loss: 0.5938449887931347\n",
      "[10, 13000] loss: 0.5845949898660183\n",
      "[10, 14000] loss: 0.595094989836216\n",
      "[10, 15000] loss: 0.5928449892997741\n",
      "[10, 16000] loss: 0.5873449896872044\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from fvs.nli.mlp import Head\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='../../runs')\n",
    "\n",
    "model = Head(embedding_length=vectors.vector_size, interaction=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimiser = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        claims, evidence, labels = data\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        outputs = model(claims, evidence)\n",
    "        loss = criterion(outputs, labels)\n",
    "        l = loss.item()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        if torch.isnan(torch.tensor(l)):\n",
    "            print(\"LOSS DTYPE IS NAN\")\n",
    "            print(\"claim \", claims)\n",
    "            print(\"evidence \", evidence)\n",
    "            print(\"output \", outputs)\n",
    "            print(\"labels \", labels)\n",
    "            print(\"loss \", loss)\n",
    "            print(list(model.parameters()))\n",
    "            break\n",
    "\n",
    "        running_loss += l\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"[{epoch+1}, {i+1}] loss: {running_loss/1000}\")\n",
    "            writer.add_scalar(\"Loss/train\", running_loss/1000, (epoch+1)*i)\n",
    "            params = list(model.parameters())\n",
    "            writer.add_histogram(\"Linear_0_grad\", params[0].grad.view(-1), global_step=(epoch+1)*i)\n",
    "            writer.add_scalar(\"Linear_0_grad_mean\", params[0].grad.view(-1).mean(), (epoch+1)*i)\n",
    "            writer.add_histogram(\"Linear_0\", params[0], global_step=(epoch+1)*i)\n",
    "            writer.add_histogram(\"Linear_1_grad\", params[2].grad.view(-1), global_step=(epoch+1)*i)\n",
    "            writer.add_histogram(\"Linear_1\", params[2], global_step=(epoch+1)*i)\n",
    "            running_loss = 0.0\n",
    "print(\"Training complete.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='../../runs')\n",
    "writer.add_scalar(\"Loss\", 1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "linear_0.weight torch.Size([128, 3])\n",
      "linear_0.bias torch.Size([128])\n",
      "linear_1.weight torch.Size([2, 128])\n",
      "linear_1.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fvs.nli.mlp import Head\n",
    "head = Head(1, True)\n",
    "for k, param in head.state_dict().items():\n",
    "    print(k, param.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}