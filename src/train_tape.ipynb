{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does changing to SGD from Adam with a lower learning rate allow it to train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "def tensorboard_scalar(writer:tf.summary.SummaryWriter, \n",
    "                       name:str, data:float, step:int):\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(name, data, step)\n",
    "        \n",
    "def tensorboard_histogram(writer:tf.summary.SummaryWriter, \n",
    "                       name:str, data:tf.Tensor, step:int):\n",
    "    with writer.as_default():\n",
    "        tf.summary.histogram(name, data, step)\n",
    "        \n",
    "def additional_notes(log_dir:str):\n",
    "    notes = input(\"Additional training notes (Press enter to skip):\")\n",
    "    if notes:\n",
    "        with open(log_dir + '/additional-notes.txt', 'w') as fh:\n",
    "            fh.write(notes)\n",
    "\n",
    "def model_summary_log(model: tf.keras.Model):\n",
    "    with open(log_dir + '/model_summary.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "def datasets_log(train_fname:str, dev_fname:str):\n",
    "    with open(log_dir + '/datasets.txt', 'w') as fh:\n",
    "        fh.write(\"Datasets used:\\n{}\\n{}\".format(suffix_train, suffix_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to log helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scalars to log ###\n",
    "def get_avg_min_max_neuron(neuron_params):\n",
    "    \"\"\" returns a tuple of constants containing the weights of a neuron. \"\"\"\n",
    "    assert len(neuron_params.shape) == 1, \"must only be for a single neuron.\"\n",
    "    \n",
    "    # get average of neuron weights\n",
    "    avg = tf.math.reduce_mean(neuron_params)\n",
    "    min_ = tf.math.reduce_min(neuron_params)\n",
    "    max_ = tf.math.reduce_max(neuron_params)\n",
    "    \n",
    "    return (avg, min_, max_)\n",
    "\n",
    "\n",
    "def get_avg_min_max_layer(layer_params_matrix):\n",
    "    \"\"\" returns a tuple of tensors containing the params (i.e. weights or grads) of a layer.\n",
    "    \n",
    "    avg -- avg[0] = average param value for each input element for neuron 0.\n",
    "    max_ -- max_[0] = max param value of every input element for neuron 0.\n",
    "    min_ -- min_[0] = min param value of every input element for neuron 0.\n",
    "    \"\"\"\n",
    "    lpm = layer_params_matrix\n",
    "    num_neurons = layer_params_matrix.shape[1]  #[0] are input elements (i.e. prev layer neurons)\n",
    "    \n",
    "    layer_info = {\n",
    "        'avg': list(),\n",
    "        'min': list(),\n",
    "        'max': list()\n",
    "    }\n",
    "    \n",
    "    # curate avg, min and max neuron weight tensors for the layer\n",
    "    for i in range(num_neurons):        \n",
    "        (n_avg, n_min, n_max) = get_avg_min_max_neuron(lpm[:, i])\n",
    "        layer_info['avg'].append(n_avg)\n",
    "        layer_info['min'].append(n_min)\n",
    "        layer_info['max'].append(n_max)\n",
    "\n",
    "    avg = tf.convert_to_tensor(layer_info['avg'])\n",
    "    min_ = tf.convert_to_tensor(layer_info['min'])\n",
    "    max_ = tf.convert_to_tensor(layer_info['max'])\n",
    "    \n",
    "    assert avg.shape[0] == num_neurons,\\\n",
    "            \"Avg neuron param tensor should be equivalent to number of neurons in the layer.\"\n",
    "    \n",
    "    return (avg, min_, max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_hist_avg_min_max_for_layer(tensors_tuple:tuple, name_suffix:str, epoch:int):\n",
    "    assert len(tensors_tuple) == 3, \"There should be average, max and min tensors.\"\n",
    "    (avg, min_, max_) = tensors_tuple\n",
    "    tensorboard_histogram(writer, '{}_avg'.format(name_suffix), avg, epoch)\n",
    "    tensorboard_histogram(writer, '{}_min'.format(name_suffix), min_, epoch)\n",
    "    tensorboard_histogram(writer, '{}_max'.format(name_suffix), max_, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length =  64\n",
    "def _parse_and_transform(serialized)-> dict:\n",
    "    feature_description = {\n",
    "        'input_word_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'target': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized, feature_description)\n",
    "    # transform\n",
    "    target = example.pop('target')\n",
    "    target = tf.reshape(target, ())\n",
    "#     target = tf.cast(target, tf.float32)\n",
    "    \n",
    "    embeddings_dict = example\n",
    "    for k, v in embeddings_dict.items():\n",
    "        embeddings_dict[k] = tf.cast(v, tf.int32)\n",
    "\n",
    "    target_dict = {'target': target}\n",
    "\n",
    "    return (embeddings_dict, target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"../dataset/tfrecords/train_64_balanced_1000_samples.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from file pattern: ../dataset/tfrecords/train_64_balanced_10000_samples.tfrecord\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Confirm? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all TFRecords with pattern ../dataset/tfrecords/train_64_balanced_10000_samples.tfrecord\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from fact_verification_system.classifier.scripts.train import _extract\n",
    "import multiprocessing\n",
    "\n",
    "suffix_train = \"train_64_balanced_10000_samples.tfrecord\"\n",
    "file_pattern = \"../dataset/tfrecords/\" + suffix_train\n",
    "\n",
    "ds_train = _extract(file_pattern)\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "ds_train = ds_train.map(_parse_and_transform, num_parallel_calls=num_cpus)\n",
    "ds_train = ds_train.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 512)          393728      keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131328      dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "target (Dense)                  (None, 1)            257         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,007,554\n",
      "Trainable params: 110,007,553\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from fact_verification_system.classifier.models.textual_entailment import create_bert_model\n",
    "model = create_bert_model(max_seq_length=64)\n",
    "\n",
    "layer_indices = {l.name: i for i, l in enumerate(model.layers)}  ## to be used layer\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Step Function ### \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Loss\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "@tf.function\n",
    "def compute_grads(train_batch: Dict[str,tf.Tensor], target_batch: tf.Tensor, \n",
    "                 loss_fn: Loss, model: tf.keras.Model):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        # forward pass\n",
    "        outputs = model(train_batch)\n",
    "        # calculate loss\n",
    "        loss = loss_fn(target_batch, outputs)\n",
    "    \n",
    "    # calculate gradients for each param\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39827d38d4a436699eaca844dbece4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epoch', max=15.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7673e19e936b4364b42357656b6f124d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6969003081321716\n",
      "average loss: 0.6949845552444458\n",
      "average loss: 0.6955817341804504\n",
      "average loss: 0.6957056522369385\n",
      "average loss: 0.6957424283027649\n",
      "average loss: 0.6902496814727783\n",
      "average loss: 0.6848475337028503\n",
      "average loss: 0.6825650930404663\n",
      "average loss: 0.6790742874145508\n",
      "\n",
      "Epoch 0: epoch_loss = 0.6789683103561401\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c64a83c1d354e79ba3cf149e1d61ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6943986415863037\n",
      "average loss: 0.6932730674743652\n",
      "average loss: 0.6933279037475586\n",
      "average loss: 0.6935257315635681\n",
      "average loss: 0.693976879119873\n",
      "average loss: 0.6884613633155823\n",
      "average loss: 0.6854684948921204\n",
      "average loss: 0.6834751963615417\n",
      "average loss: 0.6803459525108337\n",
      "\n",
      "Epoch 1: epoch_loss = 0.6789056062698364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9a83ed1dc94f8c85a1001db97c9ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6968092322349548\n",
      "average loss: 0.6956266164779663\n",
      "average loss: 0.6957470774650574\n",
      "average loss: 0.6956632137298584\n",
      "average loss: 0.6946893334388733\n",
      "average loss: 0.6884742975234985\n",
      "average loss: 0.6860034465789795\n",
      "average loss: 0.6831181645393372\n",
      "average loss: 0.6804528832435608\n",
      "\n",
      "Epoch 2: epoch_loss = 0.6799877882003784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c2762dc5f84e8786c280f672f47673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-caf79876b6c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#         set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m#         set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    439\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m           kwargs={\"name\": name})\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   1923\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    483\u001b[0m           update_ops.extend(\n\u001b[1;32m    484\u001b[0m               distribution.extended.update(\n\u001b[0;32m--> 485\u001b[0;31m                   var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   1528\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2140\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   2146\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    461\u001b[0m           \u001b[0mapply_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"apply_state\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         return self._resource_apply_sparse_duplicate_indices(\n\u001b[0;32m--> 463\u001b[0;31m             grad.values, var, grad.indices, **apply_kwargs)\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"apply_state\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/gradient_descent.py\u001b[0m in \u001b[0;36m_resource_apply_sparse_duplicate_indices\u001b[0;34m(self, grad, var, indices, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m       return resource_variable_ops.resource_scatter_add(\n\u001b[0;32m--> 137\u001b[0;31m           var.handle, indices, -grad * coefficients[\"lr_t\"])\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_resource_apply_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_scatter_add\u001b[0;34m(resource, indices, updates, name)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;34m\"ResourceScatterAdd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         indices, updates)\n\u001b[0m\u001b[1;32m    838\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Training ###\n",
    "import code\n",
    "from IPython.core.debugger import set_trace\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "assert tf.executing_eagerly(), \"Tensorflow not in eager execution mode.\"\n",
    "\n",
    "# MANUAL\n",
    "DATASET_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "\n",
    "bce = BinaryCrossentropy()\n",
    "optimizer = SGD(learning_rate=0.001)\n",
    "\n",
    "tf.random.set_seed(1)  # reproducibility -- weights initialization\n",
    "log_grads = False\n",
    "log_weights = False\n",
    "\n",
    "# tensorboard init\n",
    "timestamp = datetime.now().strftime(\"%d.%m.%Y-%H.%M.%S\")\n",
    "train_log_dir = './train_logs/{}'.format(timestamp)\n",
    "writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='epoch'):\n",
    "    # log weights\n",
    "    if log_weights: \n",
    "        dense_0_w = model.layers[-3].weights[0]\n",
    "        avg_min_max = get_avg_min_max_layer(dense_0_w)\n",
    "        tensorboard_hist_avg_min_max_for_layer(avg_min_max, 'd_0_w', epoch)\n",
    "\n",
    "        dense_1_w = model.layers[-2].weights[0]\n",
    "        avg_min_max = get_avg_min_max_layer(dense_1_w)\n",
    "        tensorboard_hist_avg_min_max_for_layer(avg_min_max, 'd_1_w', epoch)\n",
    "\n",
    "    # accumulators\n",
    "    accu_loss = 0.0\n",
    "    accu_d_0_avg_grads = tf.zeros(model.layers[layer_indices.get('dense_0')].units)\n",
    "    accu_d_1_avg_grads = tf.zeros(model.layers[layer_indices.get('dense_1')].units)\n",
    "    accu_target_grads = tf.zeros(model.layers[layer_indices.get('target')].units)\n",
    "    hist_grad_step = 0  # for marking steps on histogram\n",
    "    \n",
    "    for (i, (train_batch, target_dict)) in tqdm(enumerate(ds_train.shuffle(1024).batch(BATCH_SIZE)), desc='step'):\n",
    "\n",
    "        (grads, loss) = compute_grads(train_batch, target_dict['target'], bce, model)\n",
    "#         set_trace()\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "#         set_trace()\n",
    "        \n",
    "        accu_loss += loss\n",
    "        if (i+1) % 250 == 0:\n",
    "            print(\"average loss: {}\".format(accu_loss/(i+1)))\n",
    "        \n",
    "        # accumulate average grads\n",
    "        if log_grads:\n",
    "            steps_to_accumulate = 625\n",
    "            \n",
    "            d_0_grads = grads[-6]\n",
    "            (d_0_avg_grads, _, _) = get_avg_min_max_layer(d_0_grads)\n",
    "            accu_d_0_avg_grads += d_0_avg_grads\n",
    "            \n",
    "            d_1_grads = grads[-4]\n",
    "            (d_1_avg_grads, _, _) = get_avg_min_max_layer(d_1_grads)\n",
    "            accu_d_1_avg_grads += d_1_avg_grads\n",
    "            \n",
    "            target_grads = grads[-2]\n",
    "            accu_target_grads += target_grads\n",
    "            \n",
    "            if (i+1) % steps_to_accumulate == 0:\n",
    "                tensorboard_histogram(writer, 'd_0_grads_avg', \n",
    "                                      accu_d_0_avg_grads/steps_to_accumulate, \n",
    "                                      hist_grad_step)\n",
    "                tensorboard_histogram(writer, 'd_1_grads_avg', \n",
    "                                      accu_d_1_avg_grads/steps_to_accumulate, \n",
    "                                      hist_grad_step)\n",
    "                tensorboard_histogram(writer, 'target_grads', \n",
    "                                      accu_target_grads/steps_to_accumulate, \n",
    "                                      hist_grad_step)\n",
    "                \n",
    "                # reset grads\n",
    "                accu_d_0_avg_grads = tf.zeros(model.layers[layer_indices.get('dense_0')].units)\n",
    "                accu_d_1_avg_grads = tf.zeros(model.layers[layer_indices.get('dense_1')].units)\n",
    "                accu_target_grads = tf.zeros(model.layers[layer_indices.get('target')].units)\n",
    "                \n",
    "                hist_grad_step += 1\n",
    "\n",
    "            \n",
    "    avg_epoch_loss = accu_loss/(i+1)\n",
    "    tensorboard_scalar(writer, name='epoch_loss', data=avg_epoch_loss, step=epoch)\n",
    "    \n",
    "    print(\"Epoch {}: epoch_loss = {}\".format(epoch, avg_epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2a7e5d65354926a9a1b7adb7a431c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "for (i, (embs_dict, target_dict)) in tqdm(enumerate(ds_train.shuffle(1024).batch(8)), desc='step'):\n",
    "    outputs = model(embs_dict)\n",
    "    loss = bce(target_dict.get('target'), outputs)\n",
    "#     (grads, loss) = compute_grads(embs_dict, target_dict, bce, model)\n",
    "#     optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': 0,\n",
       " 'input_mask': 1,\n",
       " 'segment_ids': 2,\n",
       " 'keras_layer': 3,\n",
       " 'dense_0': 4,\n",
       " 'dense_1': 5,\n",
       " 'target': 6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25490, shape=(), dtype=float32, numpy=0.6917711>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
