{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length =  64\n",
    "def _parse_and_transform(serialized)-> dict:\n",
    "    feature_description = {\n",
    "        'input_word_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'target': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized, feature_description)\n",
    "    # transform\n",
    "    target = example.pop('target')\n",
    "    target = tf.reshape(target, ())\n",
    "#     target = tf.cast(target, tf.float32)\n",
    "    \n",
    "    embeddings_dict = example\n",
    "    for k, v in embeddings_dict.items():\n",
    "        embeddings_dict[k] = tf.cast(v, tf.int32)\n",
    "\n",
    "    target_dict = {'target': target}\n",
    "\n",
    "    return (embeddings_dict, target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from file pattern: ../dataset/tfrecords/train_64_balanced_1000_samples.tfrecord\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Confirm? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all TFRecords with pattern ../dataset/tfrecords/train_64_balanced_1000_samples.tfrecord\n"
     ]
    }
   ],
   "source": [
    "from fact_verification_system.classifier.scripts.train import _extract\n",
    "import multiprocessing\n",
    "\n",
    "suffix_train = \"train_64_balanced_1000_samples.tfrecord\"\n",
    "file_pattern = \"../dataset/tfrecords/\" + suffix_train\n",
    "\n",
    "ds_train = _extract(file_pattern)\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "ds_train = ds_train.map(_parse_and_transform, num_parallel_calls=num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(ds_train.batch(1)))\n",
    "train_batch = ((x[0].get('input_mask'), x[0].get('input_word_ids'), x[0].get('segment_ids')))\n",
    "target_batch = (x[1].get('target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying `tf.GradientTape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[0.55091435]]\tLabel: [1]\n",
      "tf.Tensor(0.59617573, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from fact_verification_system.classifier.models.textual_entailment import create_bert_model\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = create_bert_model(max_seq_length=64)\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # get model outputs\n",
    "    outputs = model(train_batch)\n",
    "    print(\"Output: {}\\tLabel: {}\".format(outputs, target_batch))\n",
    "    # calculate loss between targets and outputs\n",
    "    loss = bce(y_true=target_batch, y_pred=outputs)\n",
    "    print(loss)\n",
    "    \n",
    "# get gradients for each weight variable w.r.t the loss function\n",
    "grads = tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.59617573, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# binary crossentropy sanity check\n",
    "y_pred = tf.constant([[0.55091435]])\n",
    "y_true = tf.constant([[1.0]])\n",
    "loss_check = bce(y_true, y_pred)\n",
    "print(loss_check)\n",
    "assert loss_check == loss, \"Their loss should be equivalent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dense_0 = model.trainable_variables[199]\n",
    "dense_0_bias = model.trainable_variables[200]\n",
    "dense_1 = model.trainable_variables[201]\n",
    "dense_1_bias = model.trainable_variables[202]\n",
    "target = model.trainable_variables[203]\n",
    "target_bias = model.trainable_variables[204]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Inspection\n",
    "Each neuron of the layer has an **individual weight variable for each individual input element**.\n",
    "\n",
    "i.e.\n",
    "**For the one neuron, multiply each weight variable with their respective input element and sum them.**\n",
    "\n",
    "hence, for a 512 neuron layer with the previous input embedding of 768 elements, there should be 512 X 768 weight variables in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([768, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer's first neuron's weights for their respective input element.\n",
    "dense_0_w_0 = dense_0[:, 0]\n",
    "dense_0_w_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.022533596, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=42792, shape=(), dtype=float32, numpy=0.00069414685>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dense_0_w_0[0])\n",
    "tf.math.reduce_mean(dense_0_w_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying gradient to the trainable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bias_grad = grads[-1]\n",
    "target_grad = grads[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'target/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>\n",
      "tf.Tensor([-0.44908556], shape=(1,), dtype=float32)\n",
      "tf.Tensor([0.07375957], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-0.23807847], shape=(1,), dtype=float32)\n",
      "(256, 1)\n"
     ]
    }
   ],
   "source": [
    "print(target_bias)\n",
    "print(target_bias_grad)\n",
    "print(target[0])\n",
    "print(target_grad[0])\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "lr = 0.01\n",
    "optimizer = SGD(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0044908556, shape=(), dtype=float32)\n",
      "tf.Tensor([0.07614036], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "# result for target bias should become 0 - 0.01*-0.44908556\n",
    "# result for target layer's 1st weight variable should become 0.0737 - 0.01*-0.23807847 = \n",
    "print(model.trainable_variables[204][0])  # new target_bias\n",
    "print(model.trainable_variables[203][0])  # new target's 1st weight variable for the single neuron. Should have 256 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_target_bias = model.trainable_variables[204][0]\n",
    "assert new_target_bias.numpy() == (0 - lr*grads[-1]).numpy()[0], \"This should be equal to applying the gradient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([768, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[-6].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Logging Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Template\n",
    "train_log_dir = './train_logs'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "with train_summary_writer.as_default():\n",
    "    tf.summary.scalar('grad_target_bias', grads[-1][0], step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05.03.2020-03.37.38'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime(\"%d.%m.%Y-%H.%M.%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Histogram Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_summary_writer.as_default():\n",
    "    tf.summary.histogram(name='histogram_0', data=grads[-1], step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=42526, shape=(1,), dtype=float32, numpy=array([-0.44908556], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([256, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.histogram(name='histogram_1', data=grads[-2], step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([256, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find how many between -0.0289 and -0.00962\n",
    "accumulator = 0\n",
    "for g in grads[-2]:\n",
    "    if tf.math.greater(g, -0.0289) and tf.math.less(g, -0.00962): \n",
    "        accumulator += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=43389, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[-2]\n",
    "tf.math.reduce_max(grads[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=43482, shape=(), dtype=float32, numpy=-0.0054449686>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = tf.math.not_equal(grads[-2], 0)\n",
    "tf.math.reduce_max(tf.boolean_mask(grads[-2], mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=43533, shape=(14,), dtype=float32, numpy=\n",
       "array([-0.01714745, -0.01984281, -0.01187051, -0.01308818, -0.02760158,\n",
       "       -0.02324713, -0.01013472, -0.01994801, -0.01852799, -0.02048325,\n",
       "       -0.02513314, -0.01785563, -0.01581509, -0.02214511], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greater_mask = tf.math.greater(grads[-2], -0.0289)\n",
    "greater_mask\n",
    "lesser_mask = tf.math.less(grads[-2], -0.00962)\n",
    "lesser_mask\n",
    "mask = tf.math.logical_and(greater_mask, lesser_mask)\n",
    "tf.boolean_mask(grads[-2], mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 256])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-2].weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=43550, shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1, 2, 3]\n",
    "tf.convert_to_tensor(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
