{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does changing to SGD from Adam with a lower learning rate allow it to train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "def tensorboard_scalar(writer:tf.summary.SummaryWriter, \n",
    "                       name:str, data:float, step:int):\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(name, data, step)\n",
    "        \n",
    "def tensorboard_histogram(writer:tf.summary.SummaryWriter, \n",
    "                       name:str, data:tf.Tensor, step:int):\n",
    "    with writer.as_default():\n",
    "        tf.summary.histogram(name, data, step)\n",
    "        \n",
    "def additional_notes(log_dir:str):\n",
    "    notes = input(\"Additional training notes (Press enter to skip):\")\n",
    "    if notes:\n",
    "        with open(log_dir + '/additional-notes.txt', 'w') as fh:\n",
    "            fh.write(notes)\n",
    "\n",
    "def model_summary_log(model: tf.keras.Model):\n",
    "    with open(log_dir + '/model_summary.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "def datasets_log(train_fname:str, dev_fname:str):\n",
    "    with open(log_dir + '/datasets.txt', 'w') as fh:\n",
    "        fh.write(\"Datasets used:\\n{}\\n{}\".format(suffix_train, suffix_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to log helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scalars to log ###\n",
    "def get_avg_min_max_neuron(neuron_params):\n",
    "    \"\"\" returns a tuple of constants containing the weights of a neuron. \"\"\"\n",
    "    assert len(neuron_params.shape) == 1, \"must only be for a single neuron.\"\n",
    "    \n",
    "    # get average of neuron weights\n",
    "    avg = tf.math.reduce_mean(neuron_params)\n",
    "    min_ = tf.math.reduce_min(neuron_params)\n",
    "    max_ = tf.math.reduce_max(neuron_params)\n",
    "    \n",
    "    return (avg, min_, max_)\n",
    "\n",
    "\n",
    "def get_avg_min_max_layer(layer_params_matrix):\n",
    "    \"\"\" returns a tuple of tensors containing the params (i.e. weights or grads) of a layer.\n",
    "    \n",
    "    avg -- avg[0] = average param value for each input element for neuron 0.\n",
    "    max_ -- max_[0] = max param value of every input element for neuron 0.\n",
    "    min_ -- min_[0] = min param value of every input element for neuron 0.\n",
    "    \"\"\"\n",
    "    lpm = layer_params_matrix\n",
    "    num_neurons = layer_params_matrix.shape[1]  #[0] are input elements (i.e. prev layer neurons)\n",
    "    \n",
    "    layer_info = {\n",
    "        'avg': list(),\n",
    "        'min': list(),\n",
    "        'max': list()\n",
    "    }\n",
    "    \n",
    "    # curate avg, min and max neuron weight tensors for the layer\n",
    "    for i in range(num_neurons):        \n",
    "        (n_avg, n_min, n_max) = get_avg_min_max_neuron(lpm[:, i])\n",
    "        layer_info['avg'].append(n_avg)\n",
    "        layer_info['min'].append(n_min)\n",
    "        layer_info['max'].append(n_max)\n",
    "\n",
    "    avg = tf.convert_to_tensor(layer_info['avg'])\n",
    "    min_ = tf.convert_to_tensor(layer_info['min'])\n",
    "    max_ = tf.convert_to_tensor(layer_info['max'])\n",
    "    \n",
    "    assert avg.shape[0] == num_neurons,\\\n",
    "            \"Avg neuron param tensor should be equivalent to number of neurons in the layer.\"\n",
    "    \n",
    "    return (avg, min_, max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_hist_avg_min_max_for_layer(tensors_tuple:tuple, name_suffix:str, step:int):\n",
    "    assert len(tensors_tuple) == 3, \"There should be average, max and min tensors.\"\n",
    "    (avg, min_, max_) = tensors_tuple\n",
    "    tensorboard_histogram(writer, '{}_avg'.format(name_suffix), avg, step)\n",
    "    tensorboard_histogram(writer, '{}_min'.format(name_suffix), min_, step)\n",
    "    tensorboard_histogram(writer, '{}_max'.format(name_suffix), max_, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length =  64\n",
    "def _parse_and_transform(serialized)-> dict:\n",
    "    feature_description = {\n",
    "        'input_word_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'input_mask': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        'target': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized, feature_description)\n",
    "    # transform\n",
    "    target = example.pop('target')\n",
    "    target = tf.reshape(target, ())\n",
    "#     target = tf.cast(target, tf.float32)\n",
    "    \n",
    "    embeddings_dict = example\n",
    "    for k, v in embeddings_dict.items():\n",
    "        embeddings_dict[k] = tf.cast(v, tf.int32)\n",
    "\n",
    "    target_dict = {'target': target}\n",
    "\n",
    "    return (embeddings_dict, target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"../dataset/tfrecords/train_64_balanced_1000_samples.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from file pattern: ../dataset/tfrecords/train_64_balanced_10000_samples.tfrecord\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Confirm? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all TFRecords with pattern ../dataset/tfrecords/train_64_balanced_10000_samples.tfrecord\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from fact_verification_system.classifier.scripts.train import _extract\n",
    "import multiprocessing\n",
    "\n",
    "suffix_train = \"train_64_balanced_10000_samples.tfrecord\"\n",
    "file_pattern = \"../dataset/tfrecords/\" + suffix_train\n",
    "\n",
    "ds_train = _extract(file_pattern)\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "ds_train = ds_train.map(_parse_and_transform, num_parallel_calls=num_cpus)\n",
    "ds_train = ds_train.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 512)          393728      keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131328      dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "target (Dense)                  (None, 1)            257         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,007,554\n",
      "Trainable params: 110,007,553\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from fact_verification_system.classifier.models.textual_entailment import create_bert_model\n",
    "model = create_bert_model(max_seq_length=64)\n",
    "\n",
    "layer_indices = {l.name: i for i, l in enumerate(model.layers)}  ## to be used layer\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (Compute Grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for debugging training\n",
    "import tensorflow as tf\n",
    "def zero_grads_percentage(layer_grads:tf.Tensor):\n",
    "    assert len(layer_grads.shape) == 2, \"There should be gradients for each input element weight for each neuron.\"\n",
    "    \n",
    "    (avg, min_, max_) = get_avg_min_max_layer(layer_grads)\n",
    "    zero_mask = tf.math.equal(avg, 0)\n",
    "    layer_zero_grads_percent = (len(list(filter(lambda x: x, zero_mask))))/layer_grads.shape[1]\n",
    "    return layer_zero_grads_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Step Function ### \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Loss\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "@tf.function\n",
    "def compute_grads(train_batch: Dict[str,tf.Tensor], target_batch: tf.Tensor, \n",
    "                 loss_fn: Loss, model: tf.keras.Model):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        # forward pass\n",
    "        outputs = model(train_batch)\n",
    "        # calculate loss\n",
    "        loss = loss_fn(y_true=target_batch, y_pred=outputs)\n",
    "    \n",
    "    # calculate gradients for each param\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ignore this -- for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=24624, shape=(), dtype=float32, numpy=-0.78657913>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.log(0.4554)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_custom(target_batch, train_batch):\n",
    "    assert len(train_batch) == len(target_batch), \"num train and target must match.\"\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    for y_pred in train_batch:\n",
    "        y_true = tf.cast(target_batch[i], dtype=tf.float32)\n",
    "        total_loss += (y_true*tf.math.log(y_pred)) + ((1-y_true)*tf.math.log(1-y_pred))\n",
    "        i += 1\n",
    "    return total_loss/len(train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: [[0.4544241]]\n",
      "loss: 0.7887241840362549\n",
      "loss_custom: [-0.78872436]\n",
      "Loss is fine.\n",
      "205\n",
      "target: [1]\n",
      "dense_1_bias: [0.]\n",
      "dense_1_bias_grad: [-0.54557574]\n"
     ]
    }
   ],
   "source": [
    "### Testing gradient function ###\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "x_batch, target_dict = next(iter(ds_train.batch(1)))\n",
    "bce = BinaryCrossentropy()\n",
    "with tf.GradientTape(persistent=False) as tape:\n",
    "    outputs = model(x_batch)\n",
    "    print(\"outputs: {}\".format(outputs))\n",
    "    loss = bce(y_true=target_dict['target'], y_pred=outputs)\n",
    "    print(\"loss: {}\".format(loss))\n",
    "    loss_custom = bce_custom(target_dict['target'], outputs)\n",
    "    print(\"loss_custom: {}\".format(loss_custom))\n",
    "    print(\"Loss is fine.\")\n",
    "    print(len(tape.watched_variables()))\n",
    "    # loss is fine.\n",
    "\n",
    "# check gradient\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "print(\"target: {}\".format(target_dict['target']))\n",
    "print(\"dense_1_bias: {}\".format(model.layers[-1].bias.numpy()))\n",
    "print(\"dense_1_bias_grad: {}\".format(grads[-1]))\n",
    "# print(grads[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=92905, shape=(1,), dtype=float32, numpy=array([-2.2005875], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = tf.cast(target_dict['target'], dtype=tf.float32)\n",
    "y_pred = outputs[0]\n",
    "-((y_true/y_pred) - ((1-y_true)/(1-y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=92907, shape=(), dtype=float32, numpy=-29.265625>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_sum(grads[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_created_eagerly', '_persistent', '_pop_tape', '_push_tape', '_recording', '_tape', '_tf_api_names', '_tf_api_names_v1', '_watch_accessed_variables', 'batch_jacobian', 'gradient', 'jacobian', 'reset', 'stop_recording', 'watch', 'watched_variables']\n",
      "tf.Tensor([[-2.200587]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[0.4544241]])\n",
    "y_true = tf.constant([1.0])\n",
    "bce = BinaryCrossentropy()\n",
    "with tf.GradientTape(persistent=False) as tape:\n",
    "    tape.watch(x)\n",
    "    loss = bce(y_true, x)\n",
    "    print(dir(tape))\n",
    "#     print(loss)\n",
    "grad = tape.gradient(loss, x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=93206, shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "dy_dx = g.gradient(y, x)\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc32c1666f34482591b7089dd3fe61bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epoch', max=15.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e216ec5de87445ba7d54ad400c1dbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.7130096554756165\n",
      "average loss: 0.7061865925788879\n",
      "average loss: 0.7041476368904114\n",
      "average loss: 0.7020866870880127\n",
      "average loss: 0.700707733631134\n",
      "average loss: 0.6933651566505432\n",
      "average loss: 0.6903945207595825\n",
      "average loss: 0.6871565580368042\n",
      "average loss: 0.6844391822814941\n",
      "\n",
      "Epoch 0: epoch_loss = 0.6837077140808105\n",
      "0\n",
      "average dense_0 zero grad percentage: 0.6512334290468011\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e903fac9f13c4f93bfc2a80c9ea00f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6938466429710388\n",
      "average loss: 0.6944166421890259\n",
      "average loss: 0.6945149898529053\n",
      "average loss: 0.6944000720977783\n",
      "average loss: 0.6943538784980774\n",
      "average loss: 0.6888379454612732\n",
      "average loss: 0.6859286427497864\n",
      "average loss: 0.6840676069259644\n",
      "average loss: 0.6806698441505432\n",
      "\n",
      "Epoch 1: epoch_loss = 0.6803997755050659\n",
      "1\n",
      "average dense_0 zero grad percentage: 0.7132000724559897\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a33cd91350469b83fa9478a36f082d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6967105865478516\n",
      "average loss: 0.6948843598365784\n",
      "average loss: 0.6948139071464539\n",
      "average loss: 0.6946535706520081\n",
      "average loss: 0.694497287273407\n",
      "average loss: 0.6897436380386353\n",
      "average loss: 0.6854881644248962\n",
      "average loss: 0.6835901737213135\n",
      "average loss: 0.6806273460388184\n",
      "\n",
      "Epoch 2: epoch_loss = 0.6805641055107117\n",
      "2\n",
      "average dense_0 zero grad percentage: 0.7366224774581366\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aef8116cf09403d954c329c8fbc495b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6936800479888916\n",
      "average loss: 0.6937586069107056\n",
      "average loss: 0.6938440203666687\n",
      "average loss: 0.6938292980194092\n",
      "average loss: 0.6936783790588379\n",
      "average loss: 0.6897039413452148\n",
      "average loss: 0.6857832074165344\n",
      "average loss: 0.6844136714935303\n",
      "average loss: 0.6807544827461243\n",
      "\n",
      "Epoch 3: epoch_loss = 0.6802563667297363\n",
      "3\n",
      "average dense_0 zero grad percentage: 0.7792356563975955\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657a398349f6420aa9825a0f0925a40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.695762038230896\n",
      "average loss: 0.6945033669471741\n",
      "average loss: 0.6944289803504944\n",
      "average loss: 0.6942062973976135\n",
      "average loss: 0.6939288377761841\n",
      "average loss: 0.6888313889503479\n",
      "average loss: 0.6854494214057922\n",
      "average loss: 0.68318772315979\n",
      "average loss: 0.6801217198371887\n",
      "\n",
      "Epoch 4: epoch_loss = 0.679723858833313\n",
      "4\n",
      "average dense_0 zero grad percentage: 0.6953745572133964\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf2a3097eaf4a2680eb9b273c15ecbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6959211230278015\n",
      "average loss: 0.6945236325263977\n",
      "average loss: 0.6944687366485596\n",
      "average loss: 0.6942733526229858\n",
      "average loss: 0.6942042708396912\n",
      "average loss: 0.6887772083282471\n",
      "average loss: 0.6853187084197998\n",
      "average loss: 0.683218240737915\n",
      "average loss: 0.6798447370529175\n",
      "\n",
      "Epoch 5: epoch_loss = 0.6794826984405518\n",
      "5\n",
      "average dense_0 zero grad percentage: 0.7459914394589953\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210643b5dec143dfb1fbe2af52964b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6956217885017395\n",
      "average loss: 0.6945255994796753\n",
      "average loss: 0.6941826343536377\n",
      "average loss: 0.694109320640564\n",
      "average loss: 0.693950355052948\n",
      "average loss: 0.6894787549972534\n",
      "average loss: 0.6856407523155212\n",
      "average loss: 0.6832512021064758\n",
      "average loss: 0.6803297996520996\n",
      "\n",
      "Epoch 6: epoch_loss = 0.6800738573074341\n",
      "6\n",
      "average dense_0 zero grad percentage: 0.7510180737440961\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea2615ad7354de0a15a4893cd78186a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6952090859413147\n",
      "average loss: 0.6938889622688293\n",
      "average loss: 0.6936584711074829\n",
      "average loss: 0.6936056017875671\n",
      "average loss: 0.6936435103416443\n",
      "average loss: 0.6888396143913269\n",
      "average loss: 0.6852920055389404\n",
      "average loss: 0.6827411651611328\n",
      "average loss: 0.6793138980865479\n",
      "\n",
      "Epoch 7: epoch_loss = 0.6791651248931885\n",
      "7\n",
      "average dense_0 zero grad percentage: 0.7401396119579219\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb90578a63f43718d3f53a269263fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6949936747550964\n",
      "average loss: 0.6945156455039978\n",
      "average loss: 0.694232165813446\n",
      "average loss: 0.6941329836845398\n",
      "average loss: 0.6940593123435974\n",
      "average loss: 0.6881676316261292\n",
      "average loss: 0.6856600046157837\n",
      "average loss: 0.6838046312332153\n",
      "average loss: 0.6809674501419067\n",
      "\n",
      "Epoch 8: epoch_loss = 0.6805286407470703\n",
      "8\n",
      "average dense_0 zero grad percentage: 0.7134415924216402\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d50401d5bda426ca2946c21a04f9cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6948533654212952\n",
      "average loss: 0.6943889856338501\n",
      "average loss: 0.6943912506103516\n",
      "average loss: 0.6943027377128601\n",
      "average loss: 0.6943728923797607\n",
      "average loss: 0.689002275466919\n",
      "average loss: 0.684881329536438\n",
      "average loss: 0.6827519536018372\n",
      "average loss: 0.6791368126869202\n",
      "\n",
      "Epoch 9: epoch_loss = 0.6790810227394104\n",
      "9\n",
      "average dense_0 zero grad percentage: 0.6489188627093173\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250a4f7647c44ede93747f38a28a1b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6930645704269409\n",
      "average loss: 0.6931971907615662\n",
      "average loss: 0.6935930252075195\n",
      "average loss: 0.693546712398529\n",
      "average loss: 0.6936454772949219\n",
      "average loss: 0.6874518394470215\n",
      "average loss: 0.6833240389823914\n",
      "average loss: 0.6809234023094177\n",
      "average loss: 0.677990198135376\n",
      "\n",
      "Epoch 10: epoch_loss = 0.6779751777648926\n",
      "10\n",
      "average dense_0 zero grad percentage: 0.685982114104766\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da654ce2cf564bc096a981a83f98b22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6953779458999634\n",
      "average loss: 0.6943340301513672\n",
      "average loss: 0.6943866610527039\n",
      "average loss: 0.6942924857139587\n",
      "average loss: 0.694065272808075\n",
      "average loss: 0.6885058879852295\n",
      "average loss: 0.6844488978385925\n",
      "average loss: 0.6820282936096191\n",
      "average loss: 0.6787698864936829\n",
      "\n",
      "Epoch 11: epoch_loss = 0.6787858009338379\n",
      "11\n",
      "average dense_0 zero grad percentage: 0.7007466992271361\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067c196f9baf4afdb7f52450ad27002f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6931320428848267\n",
      "average loss: 0.6924148797988892\n",
      "average loss: 0.6930054426193237\n",
      "average loss: 0.6931935548782349\n",
      "average loss: 0.6932249665260315\n",
      "average loss: 0.6882827281951904\n",
      "average loss: 0.6853450536727905\n",
      "average loss: 0.6835426092147827\n",
      "average loss: 0.6798926591873169\n",
      "\n",
      "Epoch 12: epoch_loss = 0.6797335147857666\n",
      "12\n",
      "average dense_0 zero grad percentage: 0.7722886029411765\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82c21dcf2c2406db3230f2fdc09b5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6939787268638611\n",
      "average loss: 0.6934921741485596\n",
      "average loss: 0.6937398314476013\n",
      "average loss: 0.6938366293907166\n",
      "average loss: 0.6935799717903137\n",
      "average loss: 0.6888647675514221\n",
      "average loss: 0.6849583387374878\n",
      "average loss: 0.6833866834640503\n",
      "average loss: 0.6806986331939697\n",
      "\n",
      "Epoch 13: epoch_loss = 0.68007493019104\n",
      "13\n",
      "average dense_0 zero grad percentage: 0.6990678000214684\n",
      "average dense_1 zero grad percentage: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8f9e2f8b2441acbf84c1f4539aa515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.6945400834083557\n",
      "average loss: 0.6945468187332153\n",
      "average loss: 0.6945108771324158\n",
      "average loss: 0.6943650245666504\n",
      "average loss: 0.69443678855896\n",
      "average loss: 0.6887607574462891\n",
      "average loss: 0.6851092576980591\n",
      "average loss: 0.6827844977378845\n",
      "average loss: 0.6796362996101379\n",
      "\n",
      "Epoch 14: epoch_loss = 0.6794284582138062\n",
      "14\n",
      "average dense_0 zero grad percentage: 0.6469179368827823\n",
      "average dense_1 zero grad percentage: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Training ###\n",
    "import code\n",
    "from IPython.core.debugger import set_trace\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "assert tf.executing_eagerly(), \"Tensorflow not in eager execution mode.\"\n",
    "\n",
    "# MANUAL\n",
    "DATASET_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "\n",
    "bce = BinaryCrossentropy()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "# optimizer = Adam()\n",
    "\n",
    "# tf.random.set_seed(1)  # reproducibility -- weights initialization\n",
    "\n",
    "# tensorboard init\n",
    "timestamp = datetime.now().strftime(\"%d.%m.%Y-%H.%M.%S\")\n",
    "train_log_dir = './train_logs_debug/{}'.format(timestamp)\n",
    "writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "\n",
    "# log initial weights\n",
    "dense_0_w = model.layers[-3].weights[0]\n",
    "dense_0_bias = model.layers[-3].bias\n",
    "avg_min_max = get_avg_min_max_layer(dense_0_w)\n",
    "tensorboard_hist_avg_min_max_for_layer(avg_min_max, 'd_0_weights', step=0)\n",
    "tensorboard_histogram(writer, 'd_0_bias', data=dense_0_bias, step=0)\n",
    "\n",
    "dense_1_w = model.layers[-2].weights[0]\n",
    "dense_1_bias = model.layers[-2].bias\n",
    "avg_min_max = get_avg_min_max_layer(dense_1_w)\n",
    "tensorboard_hist_avg_min_max_for_layer(avg_min_max, 'd_1_weights', step=0)\n",
    "tensorboard_histogram(writer, 'd_1_bias', data=dense_1_bias, step=0)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='epoch'):\n",
    "    # - accumulators\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # - debug accumulators\n",
    "    d_0_zero_grads = 0.0\n",
    "    d_1_zero_grads = 0.0\n",
    "    \n",
    "    for (i, (train_batch, target_dict)) in tqdm(enumerate(ds_train.shuffle(1024).batch(BATCH_SIZE)), desc='step'):\n",
    "\n",
    "        (grads, loss) = compute_grads(train_batch, target_dict['target'], bce, model)\n",
    "#         print(\"outputs:\", model.layers[-2].output)\n",
    "#         tensorboard_histogram(writer, 'd_1_activations', data=model.layers[-2].output, step=(epoch+1))\n",
    "#         set_trace()\n",
    "        # debug -- track percentage of neurons receiving zero gradients\n",
    "        d_0_zero_grads += zero_grads_percentage(grads[-4])\n",
    "        d_1_zero_grads += zero_grads_percentage(grads[-2])\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        if (i+1) % 250 == 0:\n",
    "            print(\"average loss: {}\".format(epoch_loss/(i+1)))\n",
    "            \n",
    "    avg_epoch_loss = epoch_loss/(i+1)\n",
    "    tensorboard_scalar(writer, name='epoch_loss', data=avg_epoch_loss, step=epoch)\n",
    "    print(\"Epoch {}: epoch_loss = {}\".format(epoch, avg_epoch_loss))\n",
    "    print(epoch)\n",
    "\n",
    "    # % of zero gradients\n",
    "    tensorboard_scalar(writer, name='d_0_zero_grads', data=d_0_zero_grads/(i+1), step=epoch+1)\n",
    "    tensorboard_scalar(writer, name='d_1_zero_grads', data=d_1_zero_grads/(i+1), step=epoch+1)\n",
    "    print(\"average dense_0 zero grad percentage: {}\".format(d_0_zero_grads/(i+1)))\n",
    "    print(\"average dense_1 zero grad percentage: {}\".format(d_1_zero_grads/(i+1)))\n",
    "\n",
    "    dense_0_w = model.layers[-3].weights[0]\n",
    "    dense_0_bias = model.layers[-3].bias\n",
    "    avg_min_max = get_avg_min_max_layer(dense_0_w)\n",
    "    tensorboard_hist_avg_min_max_for_layer(avg_min_max, 'd_0_weights', step=(epoch+1))\n",
    "    tensorboard_histogram(writer, 'd_0_bias', data=dense_0_bias, step=(epoch+1))\n",
    "\n",
    "    dense_1_w = model.layers[-2].weights[0]\n",
    "    dense_1_bias = model.layers[-2].bias\n",
    "    avg_min_max = get_avg_min_max_layer(dense_1_w)\n",
    "    tensorboard_hist_avg_min_max_for_layer(avg_min_max, 'd_1_weights', step=(epoch+1))\n",
    "    tensorboard_histogram(writer, 'd_1_bias', data=dense_1_bias, step=(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = model.layers[-2].weights[0]\n",
    "b = model.layers[-2].weights[0]\n",
    "z = w\n",
    "model.layers[-2].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1/Identity:0' shape=(None, 256) dtype=float32>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-2].get_output_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute '_datatype_enum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_string_ops.py\u001b[0m in \u001b[0;36mstring_format\u001b[0;34m(inputs, template, placeholder, summarize, name)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;34m\"template\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"placeholder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"summarize\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         summarize)\n\u001b[0m\u001b[1;32m    812\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-e82d89ab3b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/logging_ops.py\u001b[0m in \u001b[0;36mprint_v2\u001b[0;34m(*inputs, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mplaceholder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0msummarize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         name=format_name)\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/string_ops.py\u001b[0m in \u001b[0;36mstring_format\u001b[0;34m(template, inputs, placeholder, summarize, name)\u001b[0m\n\u001b[1;32m    190\u001b[0m                                       \u001b[0mplaceholder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                                       \u001b[0msummarize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                                       name=name)\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_string_ops.py\u001b[0m in \u001b[0;36mstring_format\u001b[0;34m(inputs, template, placeholder, summarize, name)\u001b[0m\n\u001b[1;32m    815\u001b[0m         return string_format_eager_fallback(\n\u001b[1;32m    816\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             summarize=summarize, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    818\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_string_ops.py\u001b[0m in \u001b[0;36mstring_format_eager_fallback\u001b[0;34m(inputs, template, placeholder, summarize, name, ctx)\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0msummarize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m   \u001b[0msummarize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"summarize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_mixed_eager_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m   _attrs = (\"T\", _attr_T, \"template\", template, \"placeholder\", placeholder,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mconvert_to_mixed_eager_tensors\u001b[0;34m(values, ctx)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_mixed_eager_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m   \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datatype_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_mixed_eager_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m   \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datatype_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '_datatype_enum'"
     ]
    }
   ],
   "source": [
    "tf.print(model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'op'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-2d27c0ddd16d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m                                           \u001b[0;31m# input placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m          \u001b[0;31m# all layer outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfunctor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m)\u001b[0m   \u001b[0;31m# evaluation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   3771\u001b[0m       raise ValueError('Session keyword arguments are not support during '\n\u001b[1;32m   3772\u001b[0m                        'eager execution. You passed: %s' % (kwargs,))\n\u001b[0;32m-> 3773\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mEagerExecutionFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name)\u001b[0m\n\u001b[1;32m   3668\u001b[0m             \u001b[0madd_sources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3669\u001b[0m             \u001b[0mhandle_captures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3670\u001b[0;31m             base_graph=source_graph)\n\u001b[0m\u001b[1;32m   3671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3672\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlifted_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/lift_to_graph.py\u001b[0m in \u001b[0;36mlift_to_graph\u001b[0;34m(tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph, op_map)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mvisited_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/lift_to_graph.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mvisited_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'op'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functor = K.function([inp, K.learning_phase()], outputs )   # evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_TF_MODULE_IGNORED_PROPERTIES',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activity_regularizer',\n",
       " '_add_inbound_node',\n",
       " '_add_variable_with_custom_getter',\n",
       " '_autocast',\n",
       " '_call_accepts_kwargs',\n",
       " '_call_arg_was_passed',\n",
       " '_call_fn_args',\n",
       " '_callable_losses',\n",
       " '_checkpoint_dependencies',\n",
       " '_clear_losses',\n",
       " '_collect_input_masks',\n",
       " '_compute_dtype',\n",
       " '_dedup_weights',\n",
       " '_deferred_dependencies',\n",
       " '_dtype',\n",
       " '_dtype_defaulted_to_floatx',\n",
       " '_dtype_policy',\n",
       " '_dynamic',\n",
       " '_eager_add_metric',\n",
       " '_eager_losses',\n",
       " '_expects_mask_arg',\n",
       " '_expects_training_arg',\n",
       " '_flatten',\n",
       " '_gather_children_attribute',\n",
       " '_gather_saveables_for_checkpoint',\n",
       " '_get_call_arg_value',\n",
       " '_get_existing_metric',\n",
       " '_get_node_attribute_at_index',\n",
       " '_get_trainable_state',\n",
       " '_handle_activity_regularization',\n",
       " '_handle_deferred_dependencies',\n",
       " '_handle_weight_regularization',\n",
       " '_inbound_nodes',\n",
       " '_init_call_fn_args',\n",
       " '_init_set_name',\n",
       " '_initial_weights',\n",
       " '_input_spec',\n",
       " '_is_layer',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " '_layers',\n",
       " '_list_extra_dependencies_for_serialization',\n",
       " '_list_functions_for_serialization',\n",
       " '_lookup_dependency',\n",
       " '_losses',\n",
       " '_maybe_build',\n",
       " '_maybe_cast_inputs',\n",
       " '_maybe_create_attribute',\n",
       " '_maybe_initialize_trackable',\n",
       " '_metrics',\n",
       " '_name',\n",
       " '_name_based_attribute_restore',\n",
       " '_name_based_restores',\n",
       " '_name_scope',\n",
       " '_no_dependency',\n",
       " '_non_trainable_weights',\n",
       " '_obj_reference_counts',\n",
       " '_obj_reference_counts_dict',\n",
       " '_object_identifier',\n",
       " '_outbound_nodes',\n",
       " '_preload_simple_restoration',\n",
       " '_restore_from_checkpoint_position',\n",
       " '_self_name_based_restores',\n",
       " '_self_setattr_tracking',\n",
       " '_self_unconditional_checkpoint_dependencies',\n",
       " '_self_unconditional_deferred_dependencies',\n",
       " '_self_unconditional_dependency_names',\n",
       " '_self_update_uid',\n",
       " '_set_connectivity_metadata_',\n",
       " '_set_dtype_policy',\n",
       " '_set_mask_metadata',\n",
       " '_set_trainable_state',\n",
       " '_setattr_tracking',\n",
       " '_should_compute_mask',\n",
       " '_single_restoration_from_checkpoint_position',\n",
       " '_symbolic_add_metric',\n",
       " '_symbolic_call',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_thread_local',\n",
       " '_track_trackable',\n",
       " '_trackable_saved_model_saver',\n",
       " '_tracking_metadata',\n",
       " '_trainable',\n",
       " '_trainable_weights',\n",
       " '_unconditional_checkpoint_dependencies',\n",
       " '_unconditional_dependency_names',\n",
       " '_update_uid',\n",
       " '_updates',\n",
       " '_warn_about_input_casting',\n",
       " 'activation',\n",
       " 'activity_regularizer',\n",
       " 'add_loss',\n",
       " 'add_metric',\n",
       " 'add_update',\n",
       " 'add_variable',\n",
       " 'add_weight',\n",
       " 'apply',\n",
       " 'bias',\n",
       " 'bias_constraint',\n",
       " 'bias_initializer',\n",
       " 'bias_regularizer',\n",
       " 'build',\n",
       " 'built',\n",
       " 'call',\n",
       " 'compute_mask',\n",
       " 'compute_output_shape',\n",
       " 'compute_output_signature',\n",
       " 'count_params',\n",
       " 'dtype',\n",
       " 'dynamic',\n",
       " 'from_config',\n",
       " 'get_config',\n",
       " 'get_input_at',\n",
       " 'get_input_mask_at',\n",
       " 'get_input_shape_at',\n",
       " 'get_losses_for',\n",
       " 'get_output_at',\n",
       " 'get_output_mask_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_updates_for',\n",
       " 'get_weights',\n",
       " 'inbound_nodes',\n",
       " 'input',\n",
       " 'input_mask',\n",
       " 'input_shape',\n",
       " 'input_spec',\n",
       " 'kernel',\n",
       " 'kernel_constraint',\n",
       " 'kernel_initializer',\n",
       " 'kernel_regularizer',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'name',\n",
       " 'name_scope',\n",
       " 'non_trainable_variables',\n",
       " 'non_trainable_weights',\n",
       " 'outbound_nodes',\n",
       " 'output',\n",
       " 'output_mask',\n",
       " 'output_shape',\n",
       " 'set_weights',\n",
       " 'stateful',\n",
       " 'submodules',\n",
       " 'supports_masking',\n",
       " 'trainable',\n",
       " 'trainable_variables',\n",
       " 'trainable_weights',\n",
       " 'units',\n",
       " 'updates',\n",
       " 'use_bias',\n",
       " 'variables',\n",
       " 'weights',\n",
       " 'with_name_scope']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.layers[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2a7e5d65354926a9a1b7adb7a431c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='step', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "for (i, (embs_dict, target_dict)) in tqdm(enumerate(ds_train.shuffle(1024).batch(8)), desc='step'):\n",
    "    outputs = model(embs_dict)\n",
    "    loss = bce(target_dict.get('target'), outputs)\n",
    "#     (grads, loss) = compute_grads(embs_dict, target_dict, bce, model)\n",
    "#     optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': 0,\n",
       " 'input_mask': 1,\n",
       " 'segment_ids': 2,\n",
       " 'keras_layer': 3,\n",
       " 'dense_0': 4,\n",
       " 'dense_1': 5,\n",
       " 'target': 6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25490, shape=(), dtype=float32, numpy=0.6917711>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
